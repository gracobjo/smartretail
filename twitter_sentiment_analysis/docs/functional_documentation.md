# DocumentaciÃ³n Funcional - Sistema de AnÃ¡lisis de Sentimiento en Twitter

## ğŸ“‹ Ãndice

1. [DescripciÃ³n General](#descripciÃ³n-general)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [Funcionalidades Principales](#funcionalidades-principales)
4. [Pipeline de Procesamiento](#pipeline-de-procesamiento)
5. [MÃ³dulos y Componentes](#mÃ³dulos-y-componentes)
6. [API y Endpoints](#api-y-endpoints)
7. [ConfiguraciÃ³n y Uso](#configuraciÃ³n-y-uso)
8. [Casos de Uso](#casos-de-uso)
9. [Monitoreo y MÃ©tricas](#monitoreo-y-mÃ©tricas)
10. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ DescripciÃ³n General

El **Sistema de AnÃ¡lisis de Sentimiento en Twitter** es una plataforma completa que combina tecnologÃ­as modernas de NLP, procesamiento distribuido y visualizaciÃ³n interactiva para analizar la opiniÃ³n pÃºblica en redes sociales.

### Objetivos del Sistema

- **AnÃ¡lisis en Tiempo Real**: Procesamiento de tweets en streaming
- **ClasificaciÃ³n Precisa**: Uso de modelos BERT para anÃ¡lisis de sentimiento
- **Escalabilidad**: Procesamiento distribuido con PySpark
- **VisualizaciÃ³n Interactiva**: Dashboard en tiempo real
- **AnÃ¡lisis de Tendencias**: DetecciÃ³n de patrones y temas emergentes

### TecnologÃ­as Core

| TecnologÃ­a | VersiÃ³n | PropÃ³sito |
|------------|---------|-----------|
| **Python** | 3.8+ | Lenguaje principal |
| **PySpark** | 3.2+ | Procesamiento distribuido |
| **BERT/DistilBERT** | Latest | AnÃ¡lisis de sentimiento |
| **Dash/Plotly** | Latest | Dashboard interactivo |
| **Hugging Face** | Latest | Pipeline de NLP |

---

## ğŸ—ï¸ Arquitectura del Sistema

### Diagrama de Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Twitter API   â”‚â”€â”€â”€â–¶â”‚  Data Ingestion â”‚â”€â”€â”€â–¶â”‚  Preprocessing  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dashboard     â”‚â—€â”€â”€â”€â”‚  Visualization  â”‚â—€â”€â”€â”€â”‚ Sentiment Model â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Analytics     â”‚â—€â”€â”€â”€â”‚  Spark Pipeline â”‚â—€â”€â”€â”€â”‚  Data Storage   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Principales

#### 1. **Data Ingestion Layer**
- **Twitter API Integration**: Captura de tweets en tiempo real
- **Kafka Producer**: Streaming de eventos
- **Data Validation**: ValidaciÃ³n y limpieza inicial

#### 2. **Processing Layer**
- **Text Preprocessing**: Limpieza y normalizaciÃ³n de texto
- **NLP Pipeline**: TokenizaciÃ³n, embeddings, clasificaciÃ³n
- **Feature Engineering**: ExtracciÃ³n de caracterÃ­sticas

#### 3. **Analytics Layer**
- **Sentiment Analysis**: ClasificaciÃ³n de sentimientos
- **Trend Analysis**: AnÃ¡lisis de tendencias temporales
- **Topic Modeling**: DetecciÃ³n de temas emergentes

#### 4. **Visualization Layer**
- **Real-time Dashboard**: Interfaz web interactiva
- **Interactive Charts**: GrÃ¡ficos dinÃ¡micos
- **Streaming Updates**: Actualizaciones en tiempo real

---

## âš™ï¸ Funcionalidades Principales

### 1. **AnÃ¡lisis de Sentimiento Inteligente**

#### CaracterÃ­sticas del Modelo
- **Modelo Base**: BERT/DistilBERT pre-entrenado
- **ClasificaciÃ³n**: Multiclase (Positive, Negative, Neutral)
- **Confianza**: PuntuaciÃ³n de confianza para cada predicciÃ³n
- **Batch Processing**: Procesamiento eficiente por lotes

#### MÃ©tricas de Rendimiento
```python
# Ejemplo de mÃ©tricas tÃ­picas
{
    "accuracy": 0.89,
    "precision": 0.87,
    "recall": 0.85,
    "f1_score": 0.86,
    "confidence_avg": 0.78
}
```

### 2. **Procesamiento Distribuido con PySpark**

#### Capacidades de Escalabilidad
- **Procesamiento Paralelo**: DistribuciÃ³n automÃ¡tica de carga
- **Memory Management**: GestiÃ³n optimizada de memoria
- **Fault Tolerance**: Tolerancia a fallos
- **Streaming Processing**: Procesamiento en tiempo real

#### Configuraciones de Rendimiento
```yaml
spark:
  executor_memory: "4g"
  executor_cores: 2
  num_executors: 4
  spark.sql.adaptive.enabled: true
```

### 3. **Dashboard Interactivo**

#### Funcionalidades del Dashboard
- **Visualizaciones en Tiempo Real**: GrÃ¡ficos que se actualizan automÃ¡ticamente
- **Filtros DinÃ¡micos**: Filtrado por fecha, sentimiento, confianza
- **AnÃ¡lisis de Tendencias**: GrÃ¡ficos temporales
- **AnÃ¡lisis de Hashtags**: Palabras mÃ¡s frecuentes
- **Nubes de Palabras**: VisualizaciÃ³n de tÃ©rminos

#### Componentes del Dashboard
```python
# KPIs Principales
- Total de tweets procesados
- DistribuciÃ³n de sentimientos
- Promedio de confianza
- Tasa de procesamiento

# GrÃ¡ficos Interactivos
- DistribuciÃ³n de sentimientos (Pie Chart)
- LÃ­nea de tiempo temporal (Line Chart)
- AnÃ¡lisis de confianza (Histogram)
- AnÃ¡lisis de longitud vs sentimiento (Scatter)
- AnÃ¡lisis de hashtags (Bar Chart)
```

### 4. **AnÃ¡lisis de Streaming**

#### CaracterÃ­sticas de Streaming
- **Real-time Processing**: Procesamiento inmediato
- **Low Latency**: Latencia mÃ­nima (< 100ms)
- **Continuous Learning**: Aprendizaje continuo
- **Dynamic Scaling**: Escalado automÃ¡tico

#### MÃ©tricas de Streaming
```python
streaming_metrics = {
    "processed_count": 15000,
    "avg_processing_time": 0.085,
    "throughput_tweets_per_sec": 120,
    "error_rate": 0.002,
    "sentiment_distribution": {
        "positive": 0.45,
        "negative": 0.25,
        "neutral": 0.30
    }
}
```

---

## ğŸ”„ Pipeline de Procesamiento

### Flujo Completo del Pipeline

#### 1. **Data Ingestion**
```python
# Carga de datos desde mÃºltiples fuentes
def load_twitter_data(file_path: str) -> pd.DataFrame:
    """
    Carga datos de Twitter desde CSV, JSON o Parquet
    """
    if file_path.endswith('.csv'):
        return pd.read_csv(file_path)
    elif file_path.endswith('.json'):
        return pd.read_json(file_path, lines=True)
    elif file_path.endswith('.parquet'):
        return pd.read_parquet(file_path)
```

#### 2. **Text Preprocessing**
```python
# Limpieza y normalizaciÃ³n de texto
def clean_text(text: str) -> str:
    """
    Aplica limpieza completa al texto:
    - Elimina URLs
    - Normaliza menciones
    - Procesa emojis
    - Convierte a minÃºsculas
    - Elimina caracteres especiales
    """
```

#### 3. **Feature Engineering**
```python
# ExtracciÃ³n de caracterÃ­sticas
def extract_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Extrae caracterÃ­sticas adicionales:
    - Longitud del texto
    - NÃºmero de hashtags
    - NÃºmero de menciones
    - NÃºmero de URLs
    - Sentiment score
    """
```

#### 4. **Sentiment Analysis**
```python
# AnÃ¡lisis de sentimiento con BERT
def analyze_tweets(df: pd.DataFrame) -> pd.DataFrame:
    """
    Aplica modelo BERT para clasificaciÃ³n:
    - Predice sentimiento
    - Calcula confianza
    - Agrega metadatos
    """
```

#### 5. **Results Storage**
```python
# Almacenamiento de resultados
def save_results(df: pd.DataFrame, output_path: str):
    """
    Guarda resultados en mÃºltiples formatos:
    - CSV para anÃ¡lisis
    - Parquet para eficiencia
    - JSON para APIs
    """
```

### Optimizaciones del Pipeline

#### **Memory Optimization**
- **Batch Processing**: Procesamiento por lotes para optimizar memoria
- **Data Partitioning**: Particionamiento inteligente de datos
- **Caching Strategy**: Cache de resultados frecuentes

#### **Performance Optimization**
- **Parallel Processing**: Procesamiento paralelo con PySpark
- **Vectorization**: Operaciones vectorizadas con NumPy
- **GPU Acceleration**: Uso de GPU cuando estÃ¡ disponible

---

## ğŸ“¦ MÃ³dulos y Componentes

### 1. **Data Processor (`data_processor.py`)**

#### Funcionalidades Principales
```python
class TwitterDataProcessor:
    """
    Procesador principal de datos de Twitter
    """
    
    def load_twitter_data(self, file_path: str) -> pd.DataFrame:
        """Carga datos desde mÃºltiples formatos"""
        
    def clean_text(self, text: str) -> str:
        """Limpieza completa de texto"""
        
    def preprocess_tweets(self, df: pd.DataFrame) -> pd.DataFrame:
        """Preprocesamiento completo de tweets"""
        
    def extract_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ExtracciÃ³n de caracterÃ­sticas"""
        
    def create_sample_data(self, n_tweets: int = 10000) -> pd.DataFrame:
        """GeneraciÃ³n de datos de ejemplo"""
```

#### Configuraciones de Limpieza
```python
text_cleaning_config = {
    'remove_urls': True,
    'remove_mentions': True,
    'remove_hashtags': False,
    'remove_emojis': False,
    'lowercase': True,
    'remove_numbers': False,
    'min_length': 10,
    'max_length': 280
}
```

### 2. **Sentiment Analyzer (`sentiment_analyzer.py`)**

#### Modelo BERT
```python
class SentimentAnalyzer:
    """
    Analizador de sentimiento basado en BERT
    """
    
    def __init__(self, model_name: str = 'distilbert-base-uncased'):
        """
        Inicializa modelo BERT con:
        - Tokenizer optimizado
        - Modelo pre-entrenado
        - Pipeline de clasificaciÃ³n
        """
    
    def predict_sentiment(self, text: str) -> Dict:
        """
        Predice sentimiento de un texto:
        Returns: {'sentiment': 'positive', 'confidence': 0.85}
        """
    
    def analyze_tweets(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Analiza batch de tweets:
        - Aplica modelo a cada tweet
        - Calcula confianza
        - Agrega metadatos
        """
```

#### Streaming Analyzer
```python
class StreamingSentimentAnalyzer(SentimentAnalyzer):
    """
    Analizador optimizado para streaming
    """
    
    def process_stream(self, text: str) -> Dict:
        """
        Procesa un tweet en tiempo real:
        - AnÃ¡lisis inmediato
        - ActualizaciÃ³n de estadÃ­sticas
        - MÃ©tricas de rendimiento
        """
    
    def get_streaming_stats(self) -> Dict:
        """
        Obtiene estadÃ­sticas del stream:
        - Total procesado
        - DistribuciÃ³n de sentimientos
        - MÃ©tricas de rendimiento
        """
```

### 3. **Spark Pipeline (`spark_pipeline.py`)**

#### ConfiguraciÃ³n Spark
```python
class SparkSentimentPipeline:
    """
    Pipeline distribuido con PySpark
    """
    
    def __init__(self, app_name: str = "TwitterSentimentAnalysis"):
        """
        Inicializa sesiÃ³n Spark con:
        - ConfiguraciÃ³n optimizada
        - Particionamiento inteligente
        - Cache estratÃ©gico
        """
    
    def create_sample_data(self, n_tweets: int = 10000) -> None:
        """Genera datos de ejemplo distribuidos"""
    
    def preprocess_text(self, df: 'DataFrame') -> 'DataFrame':
        """Preprocesamiento distribuido de texto"""
    
    def analyze_sentiment_batch(self, df: 'DataFrame', 
                               analyzer_func) -> 'DataFrame':
        """AnÃ¡lisis de sentimiento distribuido"""
    
    def create_streaming_query(self, input_path: str, 
                              output_path: str, analyzer_func) -> 'StreamingQuery':
        """Query de streaming en tiempo real"""
```

#### Optimizaciones Spark
```python
spark_optimizations = {
    'sql.adaptive.enabled': 'true',
    'sql.adaptive.coalescePartitions.enabled': 'true',
    'sql.adaptive.skewJoin.enabled': 'true',
    'sql.adaptive.localShuffleReader.enabled': 'true',
    'sql.execution.arrow.pyspark.enabled': 'true'
}
```

### 4. **Dashboard (`dashboard.py`)**

#### Estructura del Dashboard
```python
class TwitterSentimentDashboard:
    """
    Dashboard interactivo con Dash/Plotly
    """
    
    def __init__(self):
        """
        Inicializa dashboard con:
        - Layout responsivo
        - Callbacks interactivos
        - Data loading
        """
    
    def setup_layout(self):
        """
        Configura layout del dashboard:
        - Header con tÃ­tulo
        - Filtros dinÃ¡micos
        - KPIs principales
        - GrÃ¡ficos interactivos
        """
    
    def setup_callbacks(self):
        """
        Configura callbacks para:
        - ActualizaciÃ³n de KPIs
        - Filtrado dinÃ¡mico
        - Streaming de datos
        - AnÃ¡lisis en tiempo real
        """
```

#### Componentes Visuales
```python
dashboard_components = {
    'kpi_cards': [
        'Total Tweets',
        'Positive Percentage',
        'Negative Percentage',
        'Average Confidence'
    ],
    'charts': [
        'Sentiment Distribution',
        'Confidence Distribution',
        'Sentiment Timeline',
        'Length vs Sentiment',
        'Hashtag Analysis',
        'Word Frequency'
    ],
    'filters': [
        'Date Range',
        'Sentiment Filter',
        'Confidence Threshold',
        'Text Length Range'
    ]
}
```

---

## ğŸ”Œ API y Endpoints

### REST API Endpoints

#### 1. **Sentiment Analysis**
```python
# Analizar sentimiento de un texto
POST /api/sentiment/analyze
{
    "text": "I love this product!",
    "model": "bert-base"
}

Response:
{
    "sentiment": "positive",
    "confidence": 0.89,
    "processing_time": 0.045
}
```

#### 2. **Batch Analysis**
```python
# Analizar mÃºltiples textos
POST /api/sentiment/batch
{
    "texts": ["text1", "text2", "text3"],
    "batch_size": 32
}

Response:
{
    "results": [
        {"text": "text1", "sentiment": "positive", "confidence": 0.85},
        {"text": "text2", "sentiment": "negative", "confidence": 0.78},
        {"text": "text3", "sentiment": "neutral", "confidence": 0.65}
    ],
    "total_processed": 3,
    "avg_processing_time": 0.052
}
```

#### 3. **Trends Analysis**
```python
# Obtener anÃ¡lisis de tendencias
GET /api/trends?hours=24&sentiment=positive

Response:
{
    "trends": [
        {"hour": "2024-01-15T10:00:00", "count": 150, "sentiment": "positive"},
        {"hour": "2024-01-15T11:00:00", "count": 180, "sentiment": "positive"}
    ],
    "total_tweets": 2500,
    "sentiment_distribution": {
        "positive": 0.45,
        "negative": 0.25,
        "neutral": 0.30
    }
}
```

#### 4. **Statistics**
```python
# Obtener estadÃ­sticas generales
GET /api/stats

Response:
{
    "total_tweets": 15000,
    "processed_today": 2500,
    "avg_confidence": 0.78,
    "sentiment_distribution": {
        "positive": 0.45,
        "negative": 0.25,
        "neutral": 0.30
    },
    "processing_metrics": {
        "avg_processing_time": 0.085,
        "throughput_tweets_per_sec": 120,
        "error_rate": 0.002
    }
}
```

### WebSocket API

#### 1. **Real-time Sentiment Stream**
```python
# Conectar al stream de sentimientos
ws://localhost:8000/ws/sentiment

# Mensajes recibidos
{
    "tweet_id": "123456789",
    "text": "I love this product!",
    "sentiment": "positive",
    "confidence": 0.89,
    "timestamp": "2024-01-15T10:30:00Z"
}
```

#### 2. **Trends Stream**
```python
# Conectar al stream de tendencias
ws://localhost:8000/ws/trends

# Mensajes recibidos
{
    "trend": "positive",
    "count": 150,
    "percentage": 0.45,
    "timestamp": "2024-01-15T10:30:00Z"
}
```

---

## âš™ï¸ ConfiguraciÃ³n y Uso

### InstalaciÃ³n y Setup

#### 1. **Requisitos del Sistema**
```bash
# Python 3.8+
python --version

# Apache Spark 3.2+
spark-submit --version

# Dependencias Python
pip install -r requirements.txt
```

#### 2. **ConfiguraciÃ³n de Variables de Entorno**
```bash
# .env file
TWITTER_API_KEY=your_api_key
TWITTER_API_SECRET=your_api_secret
SPARK_HOME=/path/to/spark
PYTHONPATH=/path/to/project
```

#### 3. **ConfiguraciÃ³n de Spark**
```yaml
# spark-defaults.conf
spark.driver.memory 4g
spark.executor.memory 4g
spark.executor.cores 2
spark.sql.adaptive.enabled true
spark.sql.adaptive.coalescePartitions.enabled true
```

### Comandos de EjecuciÃ³n

#### 1. **Pipeline Completo**
```bash
# Ejecutar todo el pipeline
python run_pipeline.py --mode all

# Solo configuraciÃ³n
python run_pipeline.py --setup-only
```

#### 2. **Modos EspecÃ­ficos**
```bash
# Solo generaciÃ³n de datos
python run_pipeline.py --mode data

# Solo anÃ¡lisis de sentimiento
python run_pipeline.py --mode sentiment

# Solo pipeline Spark
python run_pipeline.py --mode spark

# Solo dashboard
python run_pipeline.py --mode dashboard --port 8051

# Solo streaming
python run_pipeline.py --mode streaming
```

#### 3. **Configuraciones Avanzadas**
```bash
# Con configuraciÃ³n personalizada
python run_pipeline.py --mode all --config custom_config.yaml

# Con logging detallado
python run_pipeline.py --mode all --log-level DEBUG

# Con mÃ©tricas de rendimiento
python run_pipeline.py --mode all --profile
```

### ConfiguraciÃ³n de Modelos

#### 1. **ConfiguraciÃ³n BERT**
```python
bert_config = {
    'model_name': 'distilbert-base-uncased',
    'max_length': 128,
    'batch_size': 32,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'num_labels': 3,  # positive, negative, neutral
    'confidence_threshold': 0.7
}
```

#### 2. **ConfiguraciÃ³n Spark**
```python
spark_config = {
    'app_name': 'TwitterSentimentAnalysis',
    'master': 'local[*]',
    'executor_memory': '4g',
    'executor_cores': 2,
    'num_executors': 4,
    'spark.sql.adaptive.enabled': 'true'
}
```

#### 3. **ConfiguraciÃ³n Dashboard**
```python
dashboard_config = {
    'port': 8051,
    'debug': True,
    'host': '0.0.0.0',
    'external_stylesheets': ['bootstrap'],
    'update_interval': 5000  # ms
}
```

---

## ğŸ“Š Casos de Uso

### 1. **AnÃ¡lisis de Marca en Tiempo Real**

#### Objetivo
Monitorear la percepciÃ³n de una marca en Twitter en tiempo real.

#### ImplementaciÃ³n
```python
# Configurar filtros de marca
brand_keywords = ['@marca', '#marca', 'producto marca']

# Ejecutar anÃ¡lisis
python run_pipeline.py --mode streaming --keywords brand_keywords

# Dashboard especÃ­fico
python run_pipeline.py --mode dashboard --brand 'marca'
```

#### MÃ©tricas de InterÃ©s
- **Sentiment Score**: PuntuaciÃ³n general de sentimiento
- **Mention Volume**: Volumen de menciones
- **Sentiment Trend**: Tendencia temporal
- **Influencer Impact**: Impacto de influencers

### 2. **AnÃ¡lisis de CampaÃ±as de Marketing**

#### Objetivo
Evaluar la efectividad de campaÃ±as publicitarias.

#### ImplementaciÃ³n
```python
# Configurar hashtags de campaÃ±a
campaign_hashtags = ['#campaÃ±a2024', '#nuevoproducto']

# AnÃ¡lisis temporal
python run_pipeline.py --mode sentiment --date-range '2024-01-01,2024-01-31'

# Reporte de campaÃ±a
python generate_campaign_report.py --hashtags campaign_hashtags
```

#### KPIs de CampaÃ±a
- **Reach**: Alcance de la campaÃ±a
- **Engagement**: Nivel de engagement
- **Sentiment Shift**: Cambio en sentimiento
- **Viral Content**: Contenido viral

### 3. **AnÃ¡lisis de Competencia**

#### Objetivo
Monitorear la percepciÃ³n de competidores.

#### ImplementaciÃ³n
```python
# Configurar competidores
competitors = ['@competidor1', '@competidor2', '@competidor3']

# AnÃ¡lisis comparativo
python run_pipeline.py --mode sentiment --competitors competitors

# Dashboard comparativo
python run_pipeline.py --mode dashboard --comparison-mode
```

#### MÃ©tricas Competitivas
- **Market Share**: Cuota de mercado en conversaciones
- **Sentiment Comparison**: ComparaciÃ³n de sentimientos
- **Trend Analysis**: AnÃ¡lisis de tendencias
- **Crisis Detection**: DetecciÃ³n de crisis

### 4. **AnÃ¡lisis de Crisis**

#### Objetivo
Detectar y monitorear crisis de reputaciÃ³n.

#### ImplementaciÃ³n
```python
# Configurar alertas
crisis_keywords = ['crisis', 'problema', 'error', 'fallo']

# Monitoreo en tiempo real
python run_pipeline.py --mode streaming --alert-keywords crisis_keywords

# Dashboard de crisis
python run_pipeline.py --mode dashboard --crisis-mode
```

#### Alertas de Crisis
- **Sentiment Drop**: CaÃ­da brusca en sentimiento
- **Volume Spike**: Pico en volumen de menciones
- **Negative Trend**: Tendencia negativa sostenida
- **Influencer Impact**: Impacto de influencers

---

## ğŸ“ˆ Monitoreo y MÃ©tricas

### MÃ©tricas de Rendimiento

#### 1. **Processing Metrics**
```python
processing_metrics = {
    'throughput_tweets_per_sec': 120,
    'avg_processing_time_ms': 85,
    'batch_processing_time_ms': 250,
    'memory_usage_gb': 2.5,
    'cpu_usage_percent': 45
}
```

#### 2. **Model Performance**
```python
model_metrics = {
    'accuracy': 0.89,
    'precision': 0.87,
    'recall': 0.85,
    'f1_score': 0.86,
    'confidence_avg': 0.78,
    'confidence_std': 0.12
}
```

#### 3. **System Health**
```python
system_health = {
    'uptime_hours': 168,
    'error_rate': 0.002,
    'memory_leak': False,
    'disk_usage_percent': 65,
    'network_latency_ms': 45
}
```

### Logging y Alertas

#### 1. **Logging Configuration**
```python
import logging

logging_config = {
    'level': logging.INFO,
    'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    'handlers': [
        logging.FileHandler('logs/twitter_analysis.log'),
        logging.StreamHandler()
    ]
}
```

#### 2. **Alertas AutomÃ¡ticas**
```python
alerts_config = {
    'sentiment_drop_threshold': -0.2,
    'volume_spike_threshold': 2.0,
    'error_rate_threshold': 0.05,
    'memory_usage_threshold': 0.8
}
```

### Dashboard de Monitoreo

#### 1. **System Metrics Dashboard**
- **CPU Usage**: Uso de CPU en tiempo real
- **Memory Usage**: Uso de memoria
- **Network I/O**: Entrada/salida de red
- **Disk Usage**: Uso de disco

#### 2. **Application Metrics Dashboard**
- **Processing Throughput**: Tweets procesados por segundo
- **Model Accuracy**: PrecisiÃ³n del modelo
- **Error Rate**: Tasa de errores
- **Response Time**: Tiempo de respuesta

---

## ğŸ”§ Troubleshooting

### Problemas Comunes

#### 1. **Error de Memoria**
```python
# SÃ­ntoma: OutOfMemoryError
# SoluciÃ³n: Ajustar configuraciÃ³n de memoria

spark_config = {
    'spark.driver.memory': '8g',
    'spark.executor.memory': '8g',
    'spark.sql.adaptive.enabled': 'true'
}
```

#### 2. **Modelo Lento**
```python
# SÃ­ntoma: Procesamiento lento
# SoluciÃ³n: Optimizar modelo

optimization_config = {
    'batch_size': 64,  # Aumentar batch size
    'use_gpu': True,   # Usar GPU si estÃ¡ disponible
    'model_quantization': True  # CuantizaciÃ³n del modelo
}
```

#### 3. **Dashboard No Responde**
```python
# SÃ­ntoma: Dashboard lento o no responde
# SoluciÃ³n: Optimizar callbacks

dashboard_optimization = {
    'update_interval': 10000,  # Aumentar intervalo
    'cache_data': True,        # Cachear datos
    'lazy_loading': True       # Carga diferida
}
```

### Debugging

#### 1. **Logs de Debug**
```bash
# Habilitar logs detallados
python run_pipeline.py --mode all --log-level DEBUG

# Ver logs en tiempo real
tail -f logs/twitter_analysis.log
```

#### 2. **Profiling de Rendimiento**
```bash
# Profiling con cProfile
python -m cProfile -o profile.stats run_pipeline.py --mode all

# Analizar resultados
python -c "import pstats; pstats.Stats('profile.stats').sort_stats('cumulative').print_stats(20)"
```

#### 3. **Testing de Componentes**
```bash
# Ejecutar tests unitarios
python -m pytest tests/ -v

# Ejecutar tests de integraciÃ³n
python -m pytest tests/integration/ -v

# Ejecutar tests de rendimiento
python -m pytest tests/performance/ -v
```

### RecuperaciÃ³n de Errores

#### 1. **RecuperaciÃ³n AutomÃ¡tica**
```python
# ConfiguraciÃ³n de recuperaciÃ³n
recovery_config = {
    'auto_restart': True,
    'max_retries': 3,
    'backoff_factor': 2,
    'circuit_breaker': True
}
```

#### 2. **Backup y Restore**
```bash
# Backup de datos
python backup_data.py --backup-path /backup/

# Restore de datos
python restore_data.py --backup-path /backup/ --restore-date 2024-01-15
```

---

## ğŸ“š Referencias y Recursos

### DocumentaciÃ³n TÃ©cnica
- [PySpark Documentation](https://spark.apache.org/docs/latest/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [Dash Documentation](https://dash.plotly.com/)
- [BERT Paper](https://arxiv.org/abs/1810.04805)

### Mejores PrÃ¡cticas
- **Data Pipeline Design**: Patrones de diseÃ±o para pipelines
- **ML Model Deployment**: Despliegue de modelos de ML
- **Real-time Analytics**: Analytics en tiempo real
- **Scalable Architecture**: Arquitecturas escalables

### Comunidad y Soporte
- **GitHub Issues**: Reportar bugs y solicitar features
- **Stack Overflow**: Preguntas tÃ©cnicas
- **Discord Community**: Comunidad de desarrolladores
- **Documentation Wiki**: Wiki con ejemplos y tutoriales

---

*DocumentaciÃ³n generada automÃ¡ticamente - SmartRetail Twitter Sentiment Analysis System* 